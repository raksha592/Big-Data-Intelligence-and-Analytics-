{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clear description of the dependent and why the analysis used was chosen. (10 %)\n",
    "\n",
    "In this Analysis, we are classifying the column \"Class\" in the dataset. Hence this becomes a dependent variable.\n",
    "* Class = 1 when there has been a ClinVar variant will have conflicting classificaton. \n",
    "* Class = 0 when there is consistent classification\n",
    "\n",
    "Conflicting classifications are when two of any of the following three categories are present for one variant, two submissions of one category are not considered conflicting.\n",
    "\n",
    "1. Likely Benign or Benign\n",
    "2. VUS\n",
    "3. Likely Pathogenic or Pathogenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df =pd.read_csv(\"cleanData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the dataset, we perform feature hashing on all the columns having more unique values and one hot encode the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureHashCols=[]\n",
    "oneHotCols=[]\n",
    "df_finalFeatures=df\n",
    "for i in range(len(df.columns)):\n",
    "    if (df.dtypes[i]==object):\n",
    "        df[df.columns[i]]=df[df.columns[i]].astype('str')\n",
    "        if(len(np.unique(df[df.columns[i]]))>10):\n",
    "            X_hash = df[df.columns[i]].copy()\n",
    "            h = FeatureHasher(n_features=5,input_type=\"string\")\n",
    "            X_hash = h.transform(X_hash.values)\n",
    "            x=X_hash.toarray()\n",
    "            x=pd.DataFrame(x)\n",
    "            nameList = {}\n",
    "            for j in x.columns.values:\n",
    "                nameList[j] = df.columns[i]+str(j+1)\n",
    "\n",
    "\n",
    "            x.rename(columns = nameList, inplace = True)\n",
    "            featureHashCols.append(x)\n",
    "            df_finalFeatures=df_finalFeatures.drop([df.columns[i]],axis=1)\n",
    "        else:\n",
    "            X_oneHot=pd.get_dummies(df[df.columns[i]])\n",
    "            oneHotCols.append(X_oneHot)\n",
    "            df_finalFeatures=df_finalFeatures.drop([df.columns[i]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in featureHashCols:\n",
    "    df_finalFeatures=pd.concat([df_finalFeatures, frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in oneHotCols:\n",
    "    df_finalFeatures=pd.concat([df_finalFeatures, frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_finalFeatures.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df_.corr(), vmax=.8, square=True, cmap=cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using Logistic regression, Random forest, descion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_finalFeatures['CLASS']\n",
    "X = df_finalFeatures.drop(columns=['CLASS'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86      8400\n",
      "          1       0.00      0.00      0.00      2743\n",
      "\n",
      "avg / total       0.57      0.75      0.65     11143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred_lr = lr.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.95      0.87      8400\n",
      "          1       0.63      0.27      0.38      2743\n",
      "\n",
      "avg / total       0.76      0.78      0.75     11143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=6)\n",
    "dt.fit(X_train, y_train)\n",
    "pred_dt = dt.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.94      0.87      8400\n",
      "          1       0.64      0.30      0.41      2743\n",
      "\n",
      "avg / total       0.76      0.79      0.76     11143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform a grid search on each of the algorithms used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(0, 4, 10)\n",
    "penalty = ['l1', 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=[100,1000,5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(C=C, max_iter=max_iter, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(lr, hyperparameters, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l2'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86      8400\n",
      "          1       0.00      0.00      0.00      2743\n",
      "\n",
      "avg / total       0.57      0.75      0.65     11143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_lr=best_model.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=['gini','entropy']\n",
    "max_depth=[4,6,10,15,None]\n",
    "random_state=[10,100,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dt = dict(criterion=criterion, max_depth=max_depth, random_state=random_state)\n",
    "clf_dt = GridSearchCV(dt, hyperparameters_dt, cv=5, verbose=0)\n",
    "best_model_dt = clf_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.90      0.86      8400\n",
      "          1       0.58      0.43      0.49      2743\n",
      "\n",
      "avg / total       0.77      0.78      0.77     11143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_dt=best_model_dt.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap=[True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_rf = dict(bootstrap=bootstrap, criterion=criterion, max_depth=max_depth, random_state=random_state)\n",
    "clf_rf = GridSearchCV(rf, hyperparameters_rf, cv=5, verbose=0)\n",
    "best_model_rf = clf_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.95      0.88      8400\n",
      "          1       0.71      0.35      0.47      2743\n",
      "\n",
      "avg / total       0.79      0.80      0.78     11143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_rf=best_model_rf.predict(X_test)\n",
    "print( \"Classification Report :\\n \", classification_report(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
